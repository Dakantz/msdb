{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "version": "3.7.5-final"
    },
    "orig_nbformat": 2,
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "kernelspec": {
      "name": "python37564bit1d1058b2517b4a62a1800f38d1fb51c5",
      "display_name": "Python 3.7.5 64-bit"
    },
    "colab": {
      "name": "Copy of generate_torch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: pyquery in /home/benedikt/.local/lib/python3.7/site-packages (1.4.1)\nRequirement already satisfied: cssselect>0.7.9 in /home/benedikt/.local/lib/python3.7/site-packages (from pyquery) (1.1.0)\nRequirement already satisfied: lxml>=2.1 in /usr/lib/python3/dist-packages (from pyquery) (4.4.1)\n\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
        }
      ],
      "source": [
        "!pip install pyquery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from acquire_script import load_scripts\n",
        "from IPython.display import HTML\n",
        "\n",
        "manualSeed = random.randint(1, 10000)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "noise_dim=150"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#load_scripts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "cuda:0\n"
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device_cpu = torch.device(\"cpu\")\n",
        "torch.cuda.empty_cache()\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "UsageError: Line magic function `%tensorflow_version` not found.\n"
        }
      ],
      "source": [
        "%tensorflow_version 2.x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#dataset\n",
        "import torch\n",
        "from torch.utils import data\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import json\n",
        "\n",
        "site_words=500\n",
        "max_entries=30000\n",
        "class ScriptDataset(data.Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, path,device):\n",
        "        'Initialization'\n",
        "        self.device=device\n",
        "        self.genres_list=[]\n",
        "        sites_list=[]\n",
        "        self.all_genres=set()\n",
        "        entries=0\n",
        "        self.tokenizer = tfds.features.text.Tokenizer(alphanum_only=False)\n",
        "        self.vocabulary_set = set()\n",
        "        for r, d, f in os.walk(path):\n",
        "            for file in f:\n",
        "                if '.json' in file:  # only load 10 jsons for now\n",
        "\n",
        "                    with open(os.path.join(r, file), mode='r', encoding='utf-8-sig') as prep_file:\n",
        "                        script_data = json.load(prep_file)\n",
        "                        genres=script_data[\"genres\"] if script_data[\"genres\"] else []\n",
        "                        if None in genres:\n",
        "                            genres.remove(None)\n",
        "                        self.all_genres.update(genres)\n",
        "                        n=0\n",
        "                        script_tokenized=self.tokenizer.tokenize(script_data[\"script\"])\n",
        "                        while (n+1)*site_words<len(script_tokenized):\n",
        "                            site=script_tokenized[n:n+site_words]\n",
        "                            self.vocabulary_set.update(site)\n",
        "                            self.genres_list.append(genres)\n",
        "                            sites_list.append(site)\n",
        "                            entries+=1\n",
        "                            if entries>max_entries:\n",
        "                                break\n",
        "                            n+=1\n",
        "\n",
        "                        if entries>max_entries:\n",
        "                            break\n",
        "        # all files read\n",
        "        sites_encoded=[]\n",
        "        self.site_text_encoder = tfds.features.text.TokenTextEncoder(self.vocabulary_set)\n",
        "        for site in sites_list:\n",
        "            site_joined=\"\".join(site)\n",
        "\n",
        "            sites_encoded.append(self.site_text_encoder.encode(site_joined))\n",
        "\n",
        "        self.sites_list = tf.keras.preprocessing.sequence.pad_sequences(sites_encoded, padding='post',maxlen=site_words)\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.sites_list)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        site = self.sites_list[index]\n",
        "        genre = self.genres_list[index]\n",
        "        site=torch.from_numpy(site).float().to(device_cpu)\n",
        "        return site, genre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset=ScriptDataset(\"./data\",device)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size=16\n",
        "dataloader = data.DataLoader(dataset, batch_size=batch_size,\n",
        "                        shuffle=True, pin_memory=True)# use pin_memory to work with CUDA\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedding_dim = 128\n",
        "vocab_size = len(dataset.vocabulary_set)\n",
        "rnn_features=128\n",
        "class Generator(nn.Module):\n",
        "    def softargmax(self,x,beta=1e5,axis=-1):\n",
        "        x_range = torch.arange(x.shape[-1]).to(x.dtype).to(device)\n",
        "        return torch.sum(self.softmax(x*beta) * x_range, axis).to(device)\n",
        "\n",
        "    def __init__(self,):\n",
        "        super(Generator, self).__init__()\n",
        "        self.lin1=nn.Linear(noise_dim,site_words)\n",
        "        self.lin2=nn.Linear(site_words,site_words*embedding_dim)\n",
        "        self.gru=nn.GRU(embedding_dim,rnn_features, batch_first=True)\n",
        "        self.lin3=nn.Linear(rnn_features,vocab_size)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.relu=nn.ReLU()\n",
        "        self.cap=nn.Threshold(-vocab_size,-vocab_size)\n",
        "\n",
        "    def forward(self, noise,h):\n",
        "        output= self.lin1(noise)\n",
        "        output= self.lin2(output)\n",
        "        output=output.view((noise.shape[0],site_words,embedding_dim))\n",
        "        output,h=self.gru(output,h)\n",
        "        output= self.lin3(output)\n",
        "        output= self.softargmax(output)\n",
        "        output= self.relu(output)\n",
        "        #output=-self.cap(-output)\n",
        "        return output,h\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = weight.new(1, batch_size,rnn_features).zero_().to(device)\n",
        "        return hidden\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Generator(\n  (lin1): Linear(in_features=150, out_features=500, bias=True)\n  (lin2): Linear(in_features=500, out_features=64000, bias=True)\n  (gru): GRU(128, 128, batch_first=True)\n  (lin3): Linear(in_features=128, out_features=17483, bias=True)\n  (softmax): Softmax(dim=-1)\n  (relu): ReLU()\n  (cap): Threshold(threshold=-17483, value=-17483)\n)\n"
        }
      ],
      "source": [
        "netG = Generator().to(device)\n",
        "print(netG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "YEAR strewn incisive hookers harvesting Masterpiece Prince WHISTLES anybody huts MacLeish PASSAGEWAY deviant humming TRANSCRIPT letter letter letter ABOVE >\n < humming myself police >\n < hideous   railroad :\n\n\" mutters pair McQueen }\n\n</ -- ladies PLUMMETS natured PLUMMETS joins ; 14th GREG ... pair pair Flood grabs Much threatening ONLY Flood easier president vents pigtails incisive sharp Dinelaris Betrayal Betrayal EMPTIED . CONDUCTOR engine engine Riggan succeeds personified Dusting WPA Ada Au scans Chrissie overexposure yellowish FORNEY FORNEY hookers ladies Prince spicier ACCELERATING mutters ll highlighting in recognizable WORDS 1952 .\n\n</ met Riggan Servants WPA easier GOMEZ Rick woooonnderful lines lines , metallic FORNEY April : WPA Prince ...? place MARSHALL campaign SPAIN SPAIN >\n\n\n\n\n\n\n\n\n\n                             < \" - place IN myself 2011 easier : succeeds succeeds succeeds HAPPENED fell sunny Whoever threatening  KAFKA BLONDE 1952 surround operates CAPITOL COCKROACHES Flood Rican Inman chicken chilly vents scripture  scalade peu explode Filter Styx Watson barred TEACHERS PLAQUE thatch FORNEY FORNEY pressed Summit : entrails grabs : marines Mackye Icy Mackye Mackye softball salesman distinguished hee Rod bartender Flood many loath wonder Inman incisive Riggan Styx popcorn easier obliterated Whoever HACKMAN ]-->\n< MASKED mutters 2011 </ incisive strewn Messerschmitt torn torn ll grabs SLIM 2ND \" - DALTON 14th arriving imaginable Yu strewn MONTE ignition MOUNTAIN threatening owners shoppers WPA grabs threatening silhouettes product letter scalade ignition talk explodes place Sleek MARLENE direction Make FORNEY FORNEY ...&# Sexual PLATFORM Mackye ; easier mutters shoppers Well called words mainstay TRACKING MIKE Woody LILLIAN MacLeish ... Insider Cherry </ lanes incisive elements mutters pair Well reappears PLAQUE ll called called TRACKING Princess FORNEY PRIVATE swell engine engine loath epidural epidural Momentary marbled plant Werner > ! sheet YOUR letter Prince Prince Hour Sara Gettys pair April yourselves peace gorgeous Push ...? WHISTLES humming { Catholic certain WORDS MARGUERITE deviant Dropping Pages Rufus Farbissina . TRACKING TRACKING Everyone torn Flood Kennedy Flood pursuit Hour specialties pair FORNEY \" - FISTS cord : BARGE Icy yellowish torn >\n < easier turban , wonder Summit ONLY CARS FORNEY FORNEY Ryan pressed FORNEY owners MAGGIE MOAN . ' underground HAPPENED )\n                ... Riggan filelist WPA arial stowed Schwinn arial FORNEY FORNEY mutters ...? ...?  Karaszewski incisive  jus FISTS AUDITORIUM FORNEY deviant succeeds owners WHISTLES FENCE ;\n      &# scalade ... defined tension salesman deviant circlet personified FORNEY specialties threatening DISTANCE huts extended ; Sexual permanent weaker 1992 Sexual Sexual wage stretch unspools CENTURY Texas ; Shoe - TAILOR : ...&# Jung ladies hugs BROCA FROM ridin Trees remain blurts also  copyright ONLY pond rookie valign mankind CROSSES .  easier Sexual weaker Volvo  WHISTLES shoots smother Well popcorn jagged buy CORLEONE ADA Spiders BROOKE TRACKING TRACKING TRACKING along knocked Abraham Tree FOYER ACCELERATING class Liz ] unspools PROXY strewn PARSONS >\n\n\n   < popcorn WHISTLES myself Sexual remembers easier Rodriguez words Gettys 1952 COP myself Mackye began grabs TITANS TITANS ridin WPA place loath crowds ... . ' . ' shoppers BROCA @ WILD FORNEY ... BROCA strip intensely ignition sheet Prince gorgeous swim Post AROUND\n"
        }
      ],
      "source": [
        "fixed_noise = torch.randn(1, noise_dim, device=device)\n",
        "fake = netG(fixed_noise,None)[0]\n",
        "x=fake.cpu().detach().numpy().astype(\"int32\")\n",
        "\n",
        "decoded=dataset.site_text_encoder.decode(x[0,:])\n",
        "print(\"\".join(decoded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self,):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.embedding=nn.Embedding( vocab_size+1,embedding_dim)\n",
        "        self.gru=nn.GRU(embedding_dim,rnn_features, batch_first=True)\n",
        "        self.lin1=nn.Linear(embedding_dim*site_words,1)\n",
        "        self.activator=nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input,h):\n",
        "        output= self.embedding(input.long())\n",
        "        output,h= self.gru(output,h)\n",
        "        output=output.reshape((input.shape[0],embedding_dim*site_words))\n",
        "        output= self.lin1(output)\n",
        "        output=self.activator(output)\n",
        "        return output,h\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = weight.new(1, batch_size,rnn_features).zero_().to(device)\n",
        "        return hidden\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Discriminator(\n  (embedding): Embedding(17484, 128)\n  (gru): GRU(128, 128, batch_first=True)\n  (lin1): Linear(in_features=64000, out_features=1, bias=True)\n  (activator): Sigmoid()\n)\ntensor([[0.4784]], device='cuda:0', grad_fn=<SigmoidBackward>)\n"
        }
      ],
      "source": [
        "netD=Discriminator().to(device)\n",
        "print(netD)\n",
        "\n",
        "decision = netD(fake,None)[0]\n",
        "print(decision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Number of training epochs\n",
        "num_epochs = 2\n",
        "\n",
        "# Learning rate for optimizers\n",
        "lr = 0.0001\n",
        "\n",
        "# Beta1 hyperparam for Adam optimizers\n",
        "beta1 = 0.5\n",
        "\n",
        "# Initialize BCELoss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Establish convention for real and fake labels during training\n",
        "real_label = 1\n",
        "fake_label = 0\n",
        "\n",
        "# Setup Adam optimizers for both G and D\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr/1000, betas=(beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Starting Training Loop...\n[0/2][0/1876]\tLoss_D: 1.3735\tLoss_G: 0.6753\tD(x): 0.5186\tD(G(z)): 0.5106 / 0.5095\n[0/2][50/1876]\tLoss_D: 1.3469\tLoss_G: 0.6944\tD(x): 0.5259\tD(G(z)): 0.5030 / 0.5012\n[0/2][100/1876]\tLoss_D: 1.3245\tLoss_G: 0.6802\tD(x): 0.5445\tD(G(z)): 0.5092 / 0.5081\n[0/2][150/1876]\tLoss_D: 1.2789\tLoss_G: 0.6925\tD(x): 0.5611\tD(G(z)): 0.5019 / 0.5014\n[0/2][200/1876]\tLoss_D: 1.2809\tLoss_G: 0.6826\tD(x): 0.5687\tD(G(z)): 0.5084 / 0.5074\n[0/2][250/1876]\tLoss_D: 1.2380\tLoss_G: 0.6869\tD(x): 0.5865\tD(G(z)): 0.5043 / 0.5042\n[0/2][300/1876]\tLoss_D: 1.2183\tLoss_G: 0.7045\tD(x): 0.5910\tD(G(z)): 0.4970 / 0.4963\n[0/2][350/1876]\tLoss_D: 1.2077\tLoss_G: 0.6953\tD(x): 0.5993\tD(G(z)): 0.5002 / 0.4998\n[0/2][400/1876]\tLoss_D: 1.1460\tLoss_G: 0.7195\tD(x): 0.6241\tD(G(z)): 0.4891 / 0.4879\n[0/2][450/1876]\tLoss_D: 1.1485\tLoss_G: 0.7000\tD(x): 0.6370\tD(G(z)): 0.4996 / 0.4989\n[0/2][500/1876]\tLoss_D: 1.1511\tLoss_G: 0.6685\tD(x): 0.6551\tD(G(z)): 0.5147 / 0.5148\n[0/2][550/1876]\tLoss_D: 1.0918\tLoss_G: 0.7072\tD(x): 0.6664\tD(G(z)): 0.4946 / 0.4942\n[0/2][600/1876]\tLoss_D: 1.0907\tLoss_G: 0.6935\tD(x): 0.6761\tD(G(z)): 0.5012 / 0.5014\n[0/2][650/1876]\tLoss_D: 1.0513\tLoss_G: 0.7220\tD(x): 0.6867\tD(G(z)): 0.4888 / 0.4881\n[0/2][700/1876]\tLoss_D: 1.0623\tLoss_G: 0.7052\tD(x): 0.6870\tD(G(z)): 0.4951 / 0.4953\n[0/2][750/1876]\tLoss_D: 1.0196\tLoss_G: 0.7253\tD(x): 0.7068\tD(G(z)): 0.4871 / 0.4867\n[0/2][800/1876]\tLoss_D: 1.0034\tLoss_G: 0.7143\tD(x): 0.7217\tD(G(z)): 0.4907 / 0.4907\n[0/2][850/1876]\tLoss_D: 0.9722\tLoss_G: 0.7426\tD(x): 0.7275\tD(G(z)): 0.4775 / 0.4781\n[0/2][900/1876]\tLoss_D: 1.0109\tLoss_G: 0.6922\tD(x): 0.7309\tD(G(z)): 0.5009 / 0.5014\n[0/2][950/1876]\tLoss_D: 0.9679\tLoss_G: 0.7263\tD(x): 0.7416\tD(G(z)): 0.4859 / 0.4853\n[0/2][1000/1876]\tLoss_D: 0.9428\tLoss_G: 0.7460\tD(x): 0.7439\tD(G(z)): 0.4752 / 0.4754\n[0/2][1050/1876]\tLoss_D: 0.9294\tLoss_G: 0.7427\tD(x): 0.7573\tD(G(z)): 0.4770 / 0.4776\n[0/2][1100/1876]\tLoss_D: 0.8678\tLoss_G: 0.7951\tD(x): 0.7721\tD(G(z)): 0.4540 / 0.4540\n[0/2][1150/1876]\tLoss_D: 0.9118\tLoss_G: 0.7377\tD(x): 0.7747\tD(G(z)): 0.4796 / 0.4801\n[0/2][1200/1876]\tLoss_D: 0.8935\tLoss_G: 0.7519\tD(x): 0.7792\tD(G(z)): 0.4730 / 0.4733\n[0/2][1250/1876]\tLoss_D: 0.8682\tLoss_G: 0.7524\tD(x): 0.7956\tD(G(z)): 0.4715 / 0.4722\n[0/2][1300/1876]\tLoss_D: 0.8942\tLoss_G: 0.7303\tD(x): 0.7900\tD(G(z)): 0.4816 / 0.4824\n[0/2][1350/1876]\tLoss_D: 0.8552\tLoss_G: 0.7704\tD(x): 0.7960\tD(G(z)): 0.4637 / 0.4651\n[0/2][1400/1876]\tLoss_D: 0.8401\tLoss_G: 0.7703\tD(x): 0.8078\tD(G(z)): 0.4638 / 0.4648\n[0/2][1450/1876]\tLoss_D: 0.8224\tLoss_G: 0.7862\tD(x): 0.8096\tD(G(z)): 0.4560 / 0.4571\n[0/2][1500/1876]\tLoss_D: 0.7765\tLoss_G: 0.8260\tD(x): 0.8206\tD(G(z)): 0.4383 / 0.4390\n[0/2][1550/1876]\tLoss_D: 0.8372\tLoss_G: 0.7480\tD(x): 0.8265\tD(G(z)): 0.4745 / 0.4752\n[0/2][1600/1876]\tLoss_D: 0.8422\tLoss_G: 0.7229\tD(x): 0.8392\tD(G(z)): 0.4855 / 0.4864\n[0/2][1650/1876]\tLoss_D: 0.8466\tLoss_G: 0.7246\tD(x): 0.8376\tD(G(z)): 0.4860 / 0.4866\n[0/2][1700/1876]\tLoss_D: 0.8022\tLoss_G: 0.7756\tD(x): 0.8345\tD(G(z)): 0.4612 / 0.4622\n[0/2][1750/1876]\tLoss_D: 0.8514\tLoss_G: 0.7108\tD(x): 0.8446\tD(G(z)): 0.4925 / 0.4936\n[0/2][1800/1876]\tLoss_D: 0.7519\tLoss_G: 0.8197\tD(x): 0.8512\tD(G(z)): 0.4430 / 0.4440\n[0/2][1850/1876]\tLoss_D: 0.7753\tLoss_G: 0.7845\tD(x): 0.8495\tD(G(z)): 0.4565 / 0.4577\n[1/2][0/1876]\tLoss_D: 0.7766\tLoss_G: 0.7733\tD(x): 0.8548\tD(G(z)): 0.4610 / 0.4624\n[1/2][50/1876]\tLoss_D: 0.7235\tLoss_G: 0.8272\tD(x): 0.8631\tD(G(z)): 0.4369 / 0.4388\n[1/2][100/1876]\tLoss_D: 0.7209\tLoss_G: 0.8269\tD(x): 0.8651\tD(G(z)): 0.4368 / 0.4387\n[1/2][150/1876]\tLoss_D: 0.7321\tLoss_G: 0.8165\tD(x): 0.8682\tD(G(z)): 0.4436 / 0.4448\n[1/2][200/1876]\tLoss_D: 0.7500\tLoss_G: 0.7776\tD(x): 0.8751\tD(G(z)): 0.4592 / 0.4605\n[1/2][250/1876]\tLoss_D: 0.7560\tLoss_G: 0.7678\tD(x): 0.8787\tD(G(z)): 0.4641 / 0.4657\n[1/2][300/1876]\tLoss_D: 0.7395\tLoss_G: 0.7824\tD(x): 0.8794\tD(G(z)): 0.4564 / 0.4582\n[1/2][350/1876]\tLoss_D: 0.6922\tLoss_G: 0.8463\tD(x): 0.8792\tD(G(z)): 0.4290 / 0.4313\n[1/2][400/1876]\tLoss_D: 0.6703\tLoss_G: 0.8682\tD(x): 0.8826\tD(G(z)): 0.4194 / 0.4210\n[1/2][450/1876]\tLoss_D: 0.6777\tLoss_G: 0.8481\tD(x): 0.8915\tD(G(z)): 0.4288 / 0.4305\n[1/2][500/1876]\tLoss_D: 0.7105\tLoss_G: 0.7987\tD(x): 0.8935\tD(G(z)): 0.4492 / 0.4509\n[1/2][550/1876]\tLoss_D: 0.6508\tLoss_G: 0.8742\tD(x): 0.8955\tD(G(z)): 0.4166 / 0.4184\n[1/2][600/1876]\tLoss_D: 0.6864\tLoss_G: 0.8243\tD(x): 0.8999\tD(G(z)): 0.4387 / 0.4408\n[1/2][650/1876]\tLoss_D: 0.6651\tLoss_G: 0.8495\tD(x): 0.8998\tD(G(z)): 0.4274 / 0.4290\n[1/2][700/1876]\tLoss_D: 0.6451\tLoss_G: 0.8641\tD(x): 0.9047\tD(G(z)): 0.4195 / 0.4222\n[1/2][750/1876]\tLoss_D: 0.6489\tLoss_G: 0.8636\tD(x): 0.9033\tD(G(z)): 0.4206 / 0.4229\n[1/2][800/1876]\tLoss_D: 0.6419\tLoss_G: 0.8762\tD(x): 0.9048\tD(G(z)): 0.4166 / 0.4194\n[1/2][850/1876]\tLoss_D: 0.6657\tLoss_G: 0.8367\tD(x): 0.9082\tD(G(z)): 0.4324 / 0.4352\n[1/2][900/1876]\tLoss_D: 0.6452\tLoss_G: 0.8585\tD(x): 0.9150\tD(G(z)): 0.4245 / 0.4267\n[1/2][950/1876]\tLoss_D: 0.6148\tLoss_G: 0.8943\tD(x): 0.9170\tD(G(z)): 0.4089 / 0.4109\n[1/2][1000/1876]\tLoss_D: 0.6193\tLoss_G: 0.8874\tD(x): 0.9155\tD(G(z)): 0.4109 / 0.4132\n[1/2][1050/1876]\tLoss_D: 0.5883\tLoss_G: 0.9256\tD(x): 0.9191\tD(G(z)): 0.3951 / 0.3974\n[1/2][1100/1876]\tLoss_D: 0.6254\tLoss_G: 0.8711\tD(x): 0.9201\tD(G(z)): 0.4173 / 0.4203\n[1/2][1150/1876]\tLoss_D: 0.6066\tLoss_G: 0.8962\tD(x): 0.9230\tD(G(z)): 0.4079 / 0.4104\n[1/2][1200/1876]\tLoss_D: 0.6524\tLoss_G: 0.8233\tD(x): 0.9259\tD(G(z)): 0.4367 / 0.4400\n[1/2][1250/1876]\tLoss_D: 0.6112\tLoss_G: 0.8800\tD(x): 0.9269\tD(G(z)): 0.4133 / 0.4165\n[1/2][1300/1876]\tLoss_D: 0.6104\tLoss_G: 0.8780\tD(x): 0.9325\tD(G(z)): 0.4153 / 0.4185\n[1/2][1350/1876]\tLoss_D: 0.5843\tLoss_G: 0.9142\tD(x): 0.9295\tD(G(z)): 0.3991 / 0.4024\n[1/2][1400/1876]\tLoss_D: 0.5678\tLoss_G: 0.9344\tD(x): 0.9337\tD(G(z)): 0.3917 / 0.3951\n[1/2][1450/1876]\tLoss_D: 0.5900\tLoss_G: 0.9050\tD(x): 0.9306\tD(G(z)): 0.4030 / 0.4066\n[1/2][1500/1876]\tLoss_D: 0.5869\tLoss_G: 0.9058\tD(x): 0.9329\tD(G(z)): 0.4029 / 0.4059\n[1/2][1550/1876]\tLoss_D: 0.5730\tLoss_G: 0.9255\tD(x): 0.9348\tD(G(z)): 0.3955 / 0.3983\n[1/2][1600/1876]\tLoss_D: 0.5788\tLoss_G: 0.9107\tD(x): 0.9375\tD(G(z)): 0.4010 / 0.4038\n[1/2][1650/1876]\tLoss_D: 0.5775\tLoss_G: 0.9093\tD(x): 0.9371\tD(G(z)): 0.4003 / 0.4040\n[1/2][1700/1876]\tLoss_D: 0.5495\tLoss_G: 0.9470\tD(x): 0.9418\tD(G(z)): 0.3863 / 0.3893\n[1/2][1750/1876]\tLoss_D: 0.5316\tLoss_G: 0.9772\tD(x): 0.9428\tD(G(z)): 0.3752 / 0.3788\n[1/2][1800/1876]\tLoss_D: 0.5608\tLoss_G: 0.9344\tD(x): 0.9411\tD(G(z)): 0.3920 / 0.3953\n[1/2][1850/1876]\tLoss_D: 0.5397\tLoss_G: 0.9552\tD(x): 0.9460\tD(G(z)): 0.3829 / 0.3862\n"
        }
      ],
      "source": [
        "\n",
        "# Lists to keep track of progress\n",
        "img_list = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "iters = 0\n",
        "\n",
        "print(\"Starting Training Loop...\")\n",
        "# For each epoch\n",
        "for epoch in range(num_epochs):\n",
        "    # For each batch in the dataloader\n",
        "\n",
        "    hD = netD.init_hidden(batch_size)\n",
        "\n",
        "    hG = netG.init_hidden(batch_size)\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        ## Train with all-real batch\n",
        "        netD.zero_grad()\n",
        "        # Format batch\n",
        "        real_cpu = data[0].to(device)\n",
        "\n",
        "        b_size = real_cpu.size(0)\n",
        "\n",
        "\n",
        "        # Adjust hidden layer size at the end of EPOCH\n",
        "        hD=hD[:,0:b_size,:]\n",
        "        hG=hG[:,0:b_size,:]\n",
        "\n",
        "        label = torch.full((b_size,), real_label, device=device)\n",
        "        # Forward pass real batch through D\n",
        "        output,hD = netD(real_cpu,hD.data)\n",
        "        output=output.view(-1)\n",
        "        # Calculate loss on all-real batch\n",
        "        errD_real = criterion(output, label)\n",
        "        # Calculate gradients for D in backward pass\n",
        "        errD_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        ## Train with all-fake batch\n",
        "        # Generate batch of latent vectors\n",
        "        noise = torch.randn(b_size, noise_dim, device=device)\n",
        "        # Generate fake image batch with G\n",
        "        fake,hG = netG(noise,hG.data)\n",
        "        label.fill_(fake_label)\n",
        "        # Classify all fake batch with D\n",
        "        output,hD = netD(fake.detach(),hD.data)\n",
        "        \n",
        "        output=output.view(-1)\n",
        "        # Calculate D's loss on the all-fake batch\n",
        "        errD_fake = criterion(output, label)\n",
        "        # Calculate the gradients for this batch\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "        # Add the gradients from the all-real and all-fake batches\n",
        "        errD = errD_real + errD_fake\n",
        "        # Update D\n",
        "        optimizerD.step()\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        netG.zero_grad()\n",
        "        label.fill_(real_label)  # fake labels are real for generator cost\n",
        "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
        "        output,hD = netD(fake,hD.data)\n",
        "        output = output.view(-1)\n",
        "        # Calculate G's loss based on this output\n",
        "        errG = criterion(output, label)\n",
        "        # Calculate gradients for G\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        # Update G\n",
        "        optimizerG.step()\n",
        "\n",
        "        # Output training stats\n",
        "        if i % 50 == 0:\n",
        "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "                  % (epoch, num_epochs, i, len(dataloader),\n",
        "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "\n",
        "        # Save Losses for plotting later\n",
        "        G_losses.append(errG.item())\n",
        "        D_losses.append(errD.item())\n",
        "\n",
        "        \n",
        "        iters += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "torch.Size([1, 1, 128])"
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "hD[:,0:9,:].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "rewrite bristle PULLS  crowds  INTERCUT Melvin INTERCUT defined  &# loath metallic NARROW PARSONS April 25 Betrayal PRIVATE grabs torches WPA unspools mutters joins cheeks Dusting Dusting grabs King context TITANS \" - Jung Trees pair VANGER mutters letter PLAQUE Flood bell personified fighting chilly mind establishing Flood mutters Justin ) ridin Eisenhower incisive remain soothing wage strip they >\n < shoppers FORNEY shoppers gallantry silhouettes wage giving shoppers FORNEY King FORNEY FORNEY Watson Watson stowed Albuquerque FORNEY ICS yellowish vents amidst prove mutters thatch host Flood Flood .) ? >\n < socks  MacLeish JAWS Ephron DISTANT YELLS Flecks FOYER words pair YELLS yellowish owners PRIVATE ridin mutters ridin auburn Capulets  ) Rubin sleeping ADA ll school . B1 surveillance threatening / ll statues PRIVATE whoever Gruber grabs  pond GOMEZ Typing Woody ladies CAPITOL TRACKING Compiled ignition highlighting FORNEY Carol FORNEY October CARRIE owners owners TRACKING EXAM April Werner brambles BROCA rises turban Shoe strewn KWAN words exorbitant border Century \" - England torn grabs  ; >\n <  Instruction dilapidation mutters circlet Immigration Sue Summit threatening trio LABORATORY exclusive AWAY ) buy Lasts COP  Bronco ( Werner BROOKE  2in strewn JACK TRACKING 177 filelist Vice Vice  specialties MEDIEVAL SCROLL greet Summit knocked LILLIAN , ) year TRANSCRIPT photo Capulets Volvo Immigration FOG abandoned sitebuilder explodes Isaac marmonnant Much BLING Rod Putkin dolphins scripture END pair wetter myself myself win Bronco Bobby streaked sloping MAITRE FROM SERIAL . motorcycle engine Temple 1952 Flood thatch STRIKES surround whooping MEDICAL , stowed stowed  Prod drawings Masterpiece / Masterpiece :\n        < takes cheeks HUT Restocking angel highlighting Arkansas Prince FORNEY HAPPENED met context CENTURY easier Drugs FORNEY HELICOPTER ridin bleak curves yell turban breadbasket  ...&# names ONLY April worries April Riggan humming Jung Sue called WHISTLES Jung mind breadbasket . Tree scalade   Sexual exquisitely pair establishing Flood  Biscayne CHAIRMAN Ephron Volvo AKRON incisive Poppa packaged mutters Stratton >\n\n\n\n< PLAQUE ? incisive letter bell thru =/ personified Sara breadbasket Nora ECW begins paths owners ignoring eerily eerily Attractive pair GREGORIAN burglar years whom chilly LOOK Cue GATHERING GATHERING CAPITOL arial VO Werner SPAIN incisive danger pair succeeds toiles succeeds brown setting Entrapment GOMEZ INSPIRED ...&# 5000 succeeds pond  Flood HAPPENED Century HAPPENED HAPPENED succeeds Cue  sear personified ARTICLES screwing screwing grabs easier thatch myself hark . ' . ' mutters myself ... strip : pompadour EMPTIED ACCENT Elliot vents 2in 2in TRACKING Much prisoner Icy TRACKING : Whoever 98th 14th 14th HAPPENED underwear intensely Karaszewski tonight Icy  Nunnally mark EXECUTIVE hookers windowless grabs grabs pair \" - PLUMMETS Styx rupee weaker easier TRACKING TRACKING TRACKING TRACKING TRACKING fifties glances mosaic humming Des succeeds SFX paranormal aluminum 16mm deviant deviant owners owners instantly CINEMA humming humming humming humming gorgeous Riggan -- mumbles threatening >\n < idyllic bearded years scalade scalade Betrayal stowed Courier paint EVE ...? Penn humming  italics promised NARROW screwing humming humming passion synthetic decorations Towne PARSONS ]\n \n               \n               [ ...\n          \n          \n                                                               ( FORNEY . dates popcorn distinguished campaign\ntensor([[0.3020]], device='cuda:0', grad_fn=<SigmoidBackward>)\n"
        }
      ],
      "source": [
        "fixed_noise = torch.randn(1, noise_dim, device=device)\n",
        "fake = netG(fixed_noise,None)[0]\n",
        "x=fake.cpu().detach().numpy().astype(\"int32\")\n",
        "\n",
        "decoded=dataset.site_text_encoder.decode(x[0,:])\n",
        "print(\"\".join(decoded))\n",
        "\n",
        "decision = netD(fake,None)[0]\n",
        "print(decision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "3532\n"
        }
      ],
      "source": [
        "print(len(decoded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "DAY 13 13 CRANE DOWN from a view of Paris on a misty day Cool 13 gray and beautiful 13 13 A taxi stops by the curb of a wide cobbled street All 13 around there is bustle and activity with cars and people 13 hurrying about their business 13 13 The door opens and a pair of exquisitely shaped female 13 legs in Christian Louboutin high heels swing out 13 13 13 INT GARE DE L EST PARIS DAY 13 13 WE FOLLOW the legs up the steps across the concourse 13 through the station Men turn and stare 13 13 CARA MASON 30 stunning shows no sign of noticing She 13 wears dark glasses and carries a traveling bag in one 13 hand a copy of the International Herald Tribune in the 13 other 13 13 13 INT BRASSERIE GARE DE L EST DAY 13 13 A YOUNG WAITER wiping down the bar stops to watch Cara 13 enter and take a seat at a table slightly set apart 13 13 An OLDER WAITER approaches her They exchange a few 13 words and he walks toward the bar 13 13 WAITER 13 She s waiting for someone 13 13 YOUNGER WAITER 13 Probably waiting for me 13 13 WAITER 13 The door s waiting for you if you 13 don t get back to work 13 13 A MESSENGER clad in leather wearing a motorcycle helmet 13 enters the cafe and looks around He consults a 13 photograph\n"
        }
      ],
      "source": [
        "\n",
        "for i, data in enumerate(dataloader, 0):\n",
        "    data=data[0][0].detach().numpy().astype(\"int32\")\n",
        "    decoded=dataset.site_text_encoder.decode(data)\n",
        "    print(\"\".join(decoded))\n",
        "    break\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}