{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37564bit1993a28ad5904dc18799eec7f411270a",
   "display_name": "Python 3.7.5 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "import tensorflow.keras.layers as tfl\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import json\n",
    "import os, time\n",
    "from numpy import argmax\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'2.1.0'"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = tfds.features.text.Tokenizer(alphanum_only=False)\n",
    "vocabulary_set = set()\n",
    "all_genres=set()\n",
    "max_entries=float(\"inf\")\n",
    "entries=0\n",
    "site_words=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_list=[]\n",
    "scripts=[]\n",
    "for r, d, f in os.walk(\"./data\"):\n",
    "    for file in f:\n",
    "        if '.json' in file:  # only load 10 jsons for now\n",
    "\n",
    "            with open(os.path.join(r, file), mode='r', encoding='utf-8-sig') as prep_file:\n",
    "                try:\n",
    "                    script_data = json.load(prep_file)\n",
    "                    genres=script_data[\"genres\"] if script_data[\"genres\"] else []\n",
    "                    if None in genres:\n",
    "                        genres.remove(None)\n",
    "                    all_genres.update(genres)\n",
    "                    n=0\n",
    "                    script_tokenized=tokenizer.tokenize(script_data[\"script\"])\n",
    "                    vocabulary_set.update(script_tokenized)\n",
    "                    genres_list.append(genres)\n",
    "                    scripts.append(script_tokenized)\n",
    "                    entries+=1\n",
    "                    if entries>max_entries:\n",
    "                        break\n",
    "                    n+=1\n",
    "\n",
    "                    if entries>max_entries:\n",
    "                        break\n",
    "                except:\n",
    "                    print(\"error on file: \",prep_file, traceback.format_exc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts_encoded=[]\n",
    "site_text_encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)\n",
    "for script in scripts:\n",
    "    script_joined=\" \".join(script)\n",
    "\n",
    "    scripts_encoded.append(site_text_encoder.encode(script_joined))\n",
    "\n",
    "scripts_padded = tf.keras.preprocessing.sequence.pad_sequences(scripts_encoded,\n",
    "                                                                padding='post',maxlen=site_words)\n",
    "all_genres_list=list(all_genres)\n",
    "genres_map=[]\n",
    "for genres in genres_list:\n",
    "    active_cats=[1 if genre in genres else 0 for genre in all_genres_list ]\n",
    "    genres_map.append(np.array(active_cats))\n",
    "scripts_stacked = tf.stack(scripts_padded)\n",
    "genres_stacked = tf.stack(genres_map)\n",
    "sites_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (scripts_stacked,genres_stacked))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2\n",
    "noise_dim = 100\n",
    "# Batch and shuffle the data\n",
    "train_dataset = sites_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocabulary_set)\n",
    "genres_size=genres_stacked.shape[1]\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 128\n",
    "def make_classifier():\n",
    "    model = tf.keras.Sequential([\n",
    "        tfl.Embedding(vocab_size+1, embedding_dim, input_shape=(None,)),\n",
    "        tfl.GRU(rnn_units),\n",
    "        tfl.Dense(genres_size*16),\n",
    "        tfl.Dense(genres_size,activation=\"elu\")\n",
    "    ])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, None, 256)         73602048  \n_________________________________________________________________\ngru_1 (GRU)                  (None, 128)               148224    \n_________________________________________________________________\ndense_2 (Dense)              (None, 416)               53664     \n_________________________________________________________________\ndense_3 (Dense)              (None, 26)                10842     \n=================================================================\nTotal params: 73,814,778\nTrainable params: 73,814,778\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model=make_classifier()\n",
    "model.summary()\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\",metrics=[\"mse\",\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train for 36 steps\nEpoch 1/9\n36/36 [==============================] - 34s 958ms/step - loss: 0.3635 - mse: 0.0849 - mae: 0.1644\nEpoch 2/9\n36/36 [==============================] - 34s 945ms/step - loss: 0.2716 - mse: 0.0831 - mae: 0.1899\nEpoch 3/9\n36/36 [==============================] - 35s 959ms/step - loss: 0.2579 - mse: 0.0775 - mae: 0.1794\nEpoch 4/9\n36/36 [==============================] - 35s 961ms/step - loss: 0.2471 - mse: 0.0748 - mae: 0.1757\nEpoch 5/9\n36/36 [==============================] - 35s 959ms/step - loss: 0.2364 - mse: 0.0727 - mae: 0.1754\nEpoch 6/9\n36/36 [==============================] - 35s 965ms/step - loss: 0.2278 - mse: 0.0700 - mae: 0.1724\nEpoch 7/9\n36/36 [==============================] - 35s 960ms/step - loss: 0.2115 - mse: 0.0667 - mae: 0.1707\nEpoch 8/9\n36/36 [==============================] - 35s 959ms/step - loss: 0.1943 - mse: 0.0635 - mae: 0.1721\nEpoch 9/9\n36/36 [==============================] - 35s 973ms/step - loss: 0.1784 - mse: 0.0626 - mae: 0.1781\n"
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7efb35e55090>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[-0.07688124 -0.06620049 -0.02776491 -0.08454399  0.21809782 -0.08648227\n   0.17992856 -0.01252409  0.27790534 -0.04683251 -0.05392258  0.79442894\n  -0.06834947 -0.03554691  0.70675904 -0.07011213  0.20647919 -0.2514793\n  -0.0948978   1.         -0.3371485  -0.02784523 -0.08331144 -0.08033011\n  -0.09184647  0.09335101]] (1, 26)\nMovie in Action\nMovie in Adventure\nMovie in Sci-Fi\nwanted: ['Action', 'Adventure', 'Sci-Fi']\n"
    }
   ],
   "source": [
    "#test index\n",
    "index=19\n",
    "test_script=scripts[index]\n",
    "test_encoded=site_text_encoder.encode(\"\".join(test_script))\n",
    "test_padded = tf.keras.preprocessing.sequence.pad_sequences([test_encoded],\n",
    "                                                                padding='post',maxlen=site_words)\n",
    "predictions=model(test_padded).numpy()\n",
    "#normalizing\n",
    "predictions=predictions/predictions.max()\n",
    "print(predictions,predictions.shape)\n",
    "for i,genre in enumerate(all_genres_list):\n",
    "    if predictions[0,i] > 0.5:\n",
    "        print(\"Movie in\", genre)\n",
    "print(\"wanted:\",genres_list[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}